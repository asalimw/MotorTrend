---
title: "MotorTrend Data Analysis - EDA & Regression"
author: "Willianto Asalim"
date: "14/06/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setoptions, echo=FALSE}
## Setting Global Option where echo = true so that someone will be able to read the code and results.
knitr::opts_chunk$set(echo = TRUE, results = "hold", tidy = TRUE)
```

```{r LoadPackages, echo=FALSE}
library(knitr) ##Load Knitr package
library(ggplot2) ##Plotting and data
library(pander) ##Load package for tidy T test result
library(car)
```

## Motor Trend Data Analysis

### 1. Executive Summary

Motor Trend, a magazine about the automobile industry is looking at a data set of a collection of cars. The company is interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:

“Is an automatic or manual transmission better for MPG”
"Quantify the MPG difference between automatic and manual transmissions"

Regression models and exploratory data analyses are used to answer the above questions.

The information about the Motor Trend dataset can be found at the following link: [mtcars info](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) <-click here

<br />


### 2. Exploratory Data Analysis
***
Load the mtcars data and perform some basic exploratory data analyses
```{r mtcars}
library(datasets)
data(mtcars) ##Load mtcars dataset
```
<br />

From the fig. 1 in the appendix we know that the mtcars dataset contains `r nrow(mtcars)` observations and `r length(mtcars)` variables: `r names(mtcars)`. The fig. 2 boxplot shows the relationship between MPG and AM, from the boxplot we can see that the cars with manual transmission yield a better miles per gallon than the car with automatic transmission.


However we need to test whether it is true that the cars with manual transmission yield better mpg than cars with automatic tranmission by performing a hyphothesis testing (T Test). The null hypothesis is the transmission does not impact the result of MPG and the alternative hyphothesis is the tranmission will impact the result of MPG. To reject null hypothesis, a scientific standard of more than 95% confidence interval is used because anything less is no significance difference for scientific studies. Hence the P value (critical value) must be less than 5% ( .05) for a significance difference. If the P value is less than .05, it is likely that the transmission has impact on the MPG. If the P value is more than .05, it is unlikely that the transmission has impact on MPG. 

```{r hypothesisTest}
##T test to show whether transmission has impact on MPG
hTest <- t.test(mpg~am, data=mtcars, paired=F, var.equal=T, conf.level=0.95) 
hTest$p.value ##Getting the p value of T test 
```
The P value results of the hypothesis T test conducted shows the p value is `r hTest$p.value` which is less than .05. Therefore we reject the null hypothesis and we can conclude that tranmission type has an impact on the MPG. The T Test result of the mtcars dataset also shows that the mean for automatic is 17.15 MPG and the mean for manual is 24.39 MPG.

<br />

### 3. Regression Models Analysis
***
We need to examine further whether the AM variable is the biggest factor in determining the impact of MPG or perhaps there are other variables in the mtcars data that we should explore further.In the fig. 3 you can look at the correlation relationship between all variables.

The first regression that we will look at is the relationship between AM and MPG and in this instance we are using linear regression.

```{r linearRegression}
##Fit linear regression model
linReg <- lm(mpg ~ am, data = mtcars)
```

Please refer to the fig. 4 in the appendix for the linear regression that shows the R Square and the Adjusted R Square of 0.36 and 0.34 respectively which is pretty low. AM might not be the the best variable to determine the MPG.

The second regression will be the multivariable regression where we will include all the variables in the mtcars.
```{r multiRegression}
##Fit multiple regression model
mulReg <- lm(mpg~., data = mtcars)
```

Please refer to the fig. 5 in the appendix for the multiple regression that shows the R Square and the Adjust R Square of 0.87 and 0.81 respectively which is higher than the first linear regression. However the Variance Inflation Factor (VIF) is very high for a number of variables (more than 5) and the p Value is more than .05 for a number of variables.

The third regression will be the stepwise regression in which the choice of predictive variables is carried out by an automatic procedure. Please look at the appendix for the stepwise regression method explanation.
```{r stepwiseRegression, results='hide', error=FALSE, warning=FALSE, message=FALSE}
##Fit Stepwise regression using bidirectional method
stepReg <- step(lm(mpg~., data=mtcars), direction = "both")
```

Please refer to Fig. 6 in the appendix for the stepwise regression that shows the three variables matters to MPG namely wtm, qsec and AM. The R Square and the Adjust R Square of 0.85 and 0.835 respectively which is better than the linear and multivariable regression. The Variance Inflation Factor (VIF) is very good as it is low (less than 5) and the p Value is significance which is less than .05.

<br />

### 4. Conclusion
***
Judging 

<br />

### 5. Appendix

***
Figure 1: Data Summary
```{r dataSummary}
summary(mtcars) ##Summary of mtcars dataset
dim(mtcars) ##Number of observations and variables in mtcars dataset
names(mtcars) ##Names of the variables in mtcars dataset
```
<br />

Figure 2: Boxplot of MPG and AM relationship
```{r boxplot}
##Exploratory Data Analysis by looking at the relationship of MPG and AM using boxplot
boxplot(mpg~am, data = mtcars,
        names = c("Manual", "Automatic"),
        xlab = "Transmission",
        ylab = "Miles per Gallon",
        main = "MPG by Transmission Type")
```
<br />

Figure 3: Relationship between all variables
```{r variableRelationship}
#Chart shows relationship between all variables
pairs(mpg ~ ., data = mtcars, main="Relationships between all the variables")
cor(mtcars) ## Correlations between all variables
```
<br />

Figure 4: Linear Regression Output
```{r linearRegressionPlot}
par(mfrow = c(2,2)) ##Multiple grapths into two by two plot
plot(linReg) ##Plot linear regression model
summary(linReg) ##Output of regression model result
anova(linReg) ##Output of Analysis of Variance
```
<br />

Figure 5: Multivariable Regression Output
```{r multiRegressionPlot}
par(mfrow = c(2,2)) ##Multiple grapths into two by two plot
plot(mulReg) ##Plot multivariable regression model
summary(mulReg) ##Output of regression model result
anova(mulReg) ##Output of Analysis of Variance
vif(mulReg) ##Output of Variance Inflation Factor
```
<br />

Figure 6: Stepwise Regression Output
```{r stepwiseRegressionPlot}
par(mfrow = c(2,2)) ##Multiple grapths into two by two plot
plot(stepReg) ##Plot stepwise regression model
summary(stepReg) ##Output of regression model result
anova(stepReg) ##Output of Analysis of Variance
vif(stepReg) ##Output of Variance Inflation Factor
```
<br />

Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure.[1][2][3][4] In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Usually, this takes the form of a sequence of F-tests or t-tests, but other techniques are possible, such as adjusted R2, Akaike information criterion, Bayesian information criterion, Mallows's Cp, PRESS, or false discovery rate.

The main approaches are:

Forward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.
Backward elimination, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables can be deleted without a statistically insignificant loss of fit.
Bidirectional elimination, a combination of the above, testing at each step for variables to be included or excluded.
Source: wikipedia